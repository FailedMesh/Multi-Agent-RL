{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44771e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef5db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ma_gym:PredatorPrey5x5-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc020a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23af1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1.], (28,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8ffb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.prey_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4c4586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None, 1: None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cf015e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb008f",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d0321",
   "metadata": {},
   "source": [
    "## Environment Dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4273e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_encoding(state):\n",
    "    encoding = 0\n",
    "    for i in range(len(state)):\n",
    "        encoding += (5**(5-i))*int(state[i])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f71a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_to_state(encoding):\n",
    "    state = \"\"\n",
    "    for i in range(5, 0, -1):\n",
    "        remainder = encoding % 5\n",
    "        encoding = encoding//5\n",
    "        state = str(remainder) + state\n",
    "    remainder = encoding % 5\n",
    "    state = str(remainder) + state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40be2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_encoding(action):\n",
    "    encoding = 0\n",
    "    for i in range(len(action)):\n",
    "        encoding += (5**(1-i))*int(action[i])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a218f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_to_action(encoding):\n",
    "    action = \"\"\n",
    "    remainder = encoding % 5\n",
    "    encoding = encoding//5\n",
    "    action = str(int(remainder)) + action\n",
    "    remainder = encoding % 5\n",
    "    action = str(int(remainder)) + action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10741ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    \n",
    "    #Encoding in general means the number, state is the string\n",
    "    state = encoding_to_state(state)\n",
    "    action = encoding_to_action(action)\n",
    "    \n",
    "    good_noop = 0\n",
    "    noop_1 = 0\n",
    "    noop_2 = 0\n",
    "    \n",
    "    #STATE:\n",
    "    #First 2, Second 2, Third 2, depict the positions of the 1st pred, 2nd pred, and prey respectively.\n",
    "    \n",
    "    #PREDATOR 1:\n",
    "    \n",
    "    if action[0] == \"0\":\n",
    "        if state[0] != \"4\":\n",
    "            state = state[:0] + str(int(state[0]) + 1) + state[1:]\n",
    "    if action[0] == \"1\":\n",
    "        if state[1] != \"0\":\n",
    "            state = state[:1] + str(int(state[1]) - 1) + state[2:]\n",
    "    if action[0] == \"2\":\n",
    "        if state[0] != \"0\":\n",
    "            state = state[:0] + str(int(state[0]) - 1) + state[1:]\n",
    "    if action[0] == \"3\":\n",
    "        if state[1] != \"4\":\n",
    "            state = state[:1] + str(int(state[1]) + 1) + state[2:]\n",
    "    if action[0] == \"4\":\n",
    "        if (abs(int(state[4]) - int(state[0])) == 1 and abs(int(state[5]) - int(state[1])) == 0) or (abs(int(state[4]) - int(state[0])) == 0 and abs(int(state[5]) - int(state[1])) == 1):\n",
    "            noop_1 = 1\n",
    "        \n",
    "    #PREDATOR 2:\n",
    "    \n",
    "    if action[1] == \"0\":\n",
    "        if state[2] != \"4\":\n",
    "            state = state[:2] + str(int(state[2]) + 1) + state[3:]\n",
    "    if action[1] == \"1\":\n",
    "        if state[3] != \"0\":\n",
    "            state = state[:3] + str(int(state[3]) - 1) + state[4:]\n",
    "    if action[1] == \"2\":\n",
    "        if state[2] != \"0\":\n",
    "            state = state[:2] + str(int(state[2]) - 1) + state[3:]\n",
    "    if action[1] == \"3\":\n",
    "        if state[3] != \"4\":\n",
    "            state = state[:3] + str(int(state[3]) + 1) + state[4:]\n",
    "    if action[1] == \"4\":\n",
    "        if (abs(int(state[4]) - int(state[2])) == 1 and abs(int(state[5]) - int(state[3])) == 0) or (abs(int(state[4]) - int(state[2])) == 0 and abs(int(state[5]) - int(state[3])) == 1):\n",
    "            noop_2 = 1\n",
    "            \n",
    "    reward_array = np.zeros((4,))\n",
    "    state_array = [state, state, state, state]\n",
    "    prev_state_array = state_array[:]\n",
    "    \n",
    "    #All Possible Next States:\n",
    "    \n",
    "    if state_array[0][4] != \"4\":\n",
    "        state_array[0] = state_array[0][:4] + str(int(state_array[0][4]) + 1) + state_array[0][5:]\n",
    "    if state_array[1][5] != \"0\":\n",
    "        state_array[1] = state_array[1][:5] + str(int(state_array[1][5]) - 1)\n",
    "    if state_array[2][4] != \"0\":\n",
    "        state_array[2] = state_array[2][:4] + str(int(state_array[2][4]) - 1) + state_array[2][5:]\n",
    "    if state_array[3][5] != \"4\":\n",
    "        state_array[3] = state_array[3][:5] + str(int(state_array[3][5]) + 1)\n",
    "    \n",
    "            \n",
    "    #TERMINAL CASES:\n",
    "    \n",
    "    for i in range(len(state_array)):\n",
    "\n",
    "#         if action[0] == \"4\":\n",
    "#             if abs(int(state_array[i][4]) - int(state_array[i][0])) == 1 or abs(int(state_array[i][5]) - int(state_array[i][1])) == 1:\n",
    "#                 noop_1 = 1\n",
    "\n",
    "#         if action[1] == \"4\":\n",
    "#             if abs(int(state_array[i][4]) - int(state_array[i][2])) == 1 or abs(int(state_array[i][5]) - int(state_array[i][3])) == 1:\n",
    "#                 noop_2 = 1\n",
    "        \n",
    "        if (noop_1 + noop_2) == 2:          #Correct Noop\n",
    "            state_array[i] = state_to_encoding(state_array[i])\n",
    "            reward_array[i] = 1\n",
    "        elif (noop_1 + noop_2) == 1:        #Wrong Noop       \n",
    "            state_array[i] = state_to_encoding(state_array[i])\n",
    "            reward_array[i] = -0.5\n",
    "        elif (action[0] == \"4\" and noop_1 == 0) or (action[1] == \"4\" and noop_2 == 0):  #Terrible Noop\n",
    "            state_array[i] = state_to_encoding(state_array[i])\n",
    "            reward_array[i] = -1\n",
    "        #Collision:\n",
    "        elif (state_array[i][:2] == state_array[i][2:4]) or (state_array[i][2:4] == state_array[i][4:]) or (state_array[i][:2] == state_array[i][4:]):\n",
    "            state_array[i] = state_to_encoding(prev_state_array[:][i])\n",
    "            reward_array[i] = -0.5\n",
    "        #Movement Collision:\n",
    "        elif (state_array[i][:2] == prev_state_array[i][2:4] and prev_state_array[i][:2] == state_array[i][2:4]) or (state_array[i][4:] == prev_state_array[i][2:4] and prev_state_array[i][4:] == state_array[i][2:4]) or (state_array[i][:2] == prev_state_array[i][4:] and prev_state_array[i][:2] == state_array[i][4:]):\n",
    "            state_array[i] = state_to_encoding(prev_state_array[:][i])\n",
    "            reward_array[i] = -0.5\n",
    "        else:\n",
    "            state_array[i] = state_to_encoding(state_array[i])\n",
    "            reward_array[i] = -0.01\n",
    "        \n",
    "    \n",
    "    return reward_array, state_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4d0130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4544, 4538, 4534, 4539] [-0.01 -0.01 -0.01 -0.01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'121123'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_action = action_to_encoding(\"31\")\n",
    "encoded_state = state_to_encoding(\"111224\")\n",
    "reward, next_state = get_next_state(encoded_state, encoded_action)\n",
    "print(next_state, reward)\n",
    "encoding_to_state(next_state[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b161d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = \"001144\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf3024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0-->Down\n",
    "#1-->Left\n",
    "#2-->Up\n",
    "#3-->Right\n",
    "action_dict = {0:\"Down\", 1:\"Left\", 2:\"Up\", 3:\"Right\", 4:\"Noop\"}\n",
    "\n",
    "encoded_state = state_to_encoding(state)\n",
    "encoded_action = action_to_encoding(\"20\")\n",
    "state = get_next_state(encoded_state, encoded_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e50b7a",
   "metadata": {},
   "source": [
    "## Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "278b3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.n = len(self.env.observation_space) + 1              # 1 prey\n",
    "        self.num_states = 5**(2*self.n)    # 5 rows/columns squared for area and multiplied by number of agents including prey\n",
    "        self.num_actions = 5*5\n",
    "        self.values = np.random.rand(self.num_states)            \n",
    "        self.policy = np.zeros(self.num_states, )\n",
    "        \n",
    "        \n",
    "    def value_iterate(self, theta = 0.01, gamma = 0.5, save = False):\n",
    "        \n",
    "        while True:\n",
    "            delV = 0\n",
    "            for state in range(self.num_states):\n",
    "                prev_value = self.values[state]\n",
    "                action_values = []\n",
    "                for action in range(self.num_actions):\n",
    "                    reward_array, next_state_array = get_next_state(state, action)\n",
    "                    next_state_vals = 0\n",
    "                    for i in range(len(next_state_array)):\n",
    "                        if next_state_array[i] == -1:\n",
    "                            next_state_vals += (0.25*reward_array[i])\n",
    "                        else:\n",
    "                            next_state_vals += (0.25*(reward_array[i] + gamma*self.values[next_state_array[i]]))\n",
    "                    action_values.append(next_state_vals)\n",
    "                self.values[state] = max(action_values)\n",
    "                delV = max(delV, abs(prev_value - self.values[state]))\n",
    "            print(delV)\n",
    "            if delV < theta:\n",
    "                #print(action_values)\n",
    "                break\n",
    "            \n",
    "                \n",
    "        for state in range(self.num_states):\n",
    "            action_values = []\n",
    "            for action in range(self.num_actions):\n",
    "                reward_array, next_state_array = get_next_state(state, action)\n",
    "                next_state_vals = 0\n",
    "                for i in range(len(next_state_array)):\n",
    "                    if next_state_array[i] == -1:\n",
    "                        next_state_vals += (0.25*reward_array[i])\n",
    "                    else:\n",
    "                        next_state_vals += (0.25*(reward_array[i] + gamma*self.values[next_state_array[i]]))\n",
    "                action_values.append(next_state_vals)\n",
    "            action_values = np.array(action_values)\n",
    "            self.policy[state] = np.argmax(action_values)\n",
    "            #print(action_values, self.policy[state])\n",
    "        \n",
    "        if save:\n",
    "            np.save(\"DP_policy.npy\", self.policy)\n",
    "\n",
    "                \n",
    "    def analyse_episode(self):\n",
    "        \n",
    "        dones = [False for _ in range(self.env.n_agents)]\n",
    "        action = [0, 0]\n",
    "        reward = [0, 0]\n",
    "        my_reward = [0, 0]\n",
    "        info = None\n",
    "        \n",
    "        _ = self.env.reset()\n",
    "        count = 0\n",
    "        \n",
    "        self.env.render()\n",
    "        _ = input(\"Press Enter to start simulation\")\n",
    "        \n",
    "        while not all(dones):\n",
    "            \n",
    "            #clear_output(wait = True)\n",
    "            self.env.render()\n",
    "            \n",
    "            a1_pos = str(self.env.agent_pos[0][0]) + str(self.env.agent_pos[0][1])\n",
    "            a2_pos = str(self.env.agent_pos[1][0]) + str(self.env.agent_pos[1][1])\n",
    "            prey_pos = str(self.env.prey_pos[0][0]) + str(self.env.prey_pos[0][1])\n",
    "            \n",
    "            state_string = a1_pos + a2_pos + prey_pos\n",
    "            state_encoding = state_to_encoding(state_string)\n",
    "            action_encoding = self.policy[state_encoding]\n",
    "            action_string = encoding_to_action(action_encoding)\n",
    "            action[0], action[1] = int(action_string[0]), int(action_string[1])\n",
    "            print(\" \")\n",
    "            print(\"-----------------------------------------------------------------------------------\")\n",
    "            print(action_dict[action[0]], action_dict[action[1]])\n",
    "            print(\"Current state in their representation = \", state_string)\n",
    "            my_reward, my_next_state = get_next_state(state_encoding, action_encoding)\n",
    "            _ = input(\"Does the action make sense? Press Enter to execute\")\n",
    "            \n",
    "            _, reward, dones, info = self.env.step(action)\n",
    "            \n",
    "            print(\"REWARD = \", my_reward)\n",
    "            print(\"Is agent done?: \", dones)\n",
    "            self.env.render()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0170461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31001bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15. 24. 18. ...  6. 24.  7.]\n"
     ]
    }
   ],
   "source": [
    "agent.policy = np.load(\"DP_policy.npy\")\n",
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96fceef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7815917622652238\n",
      "0.8808317273182067\n",
      "0.748936410974173\n",
      "0.5195209758163374\n",
      "0.4146785777410069\n",
      "0.32743105044456455\n",
      "0.2679261496155172\n",
      "0.231403068569644\n",
      "0.19869120725867462\n",
      "0.16963945073456532\n",
      "0.14278840215912547\n",
      "0.1172898988727602\n",
      "0.09615829968708578\n",
      "0.07844519276945894\n",
      "0.06368568860644919\n",
      "0.054258567183148365\n",
      "0.046693814871447525\n",
      "0.040120067700834205\n",
      "0.034449886878050506\n",
      "0.02953532600179276\n",
      "0.025294542535058362\n",
      "0.0216335434691155\n",
      "0.018480459808377425\n",
      "0.015767574625231262\n",
      "0.013437096057771214\n",
      "0.011437785183667337\n",
      "0.009725035305041096\n"
     ]
    }
   ],
   "source": [
    "agent.value_iterate(theta = 0.01, gamma = 0.9, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8adccd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"DP_policy_gamma001.npy\", agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cc5d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fa8d9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Enter to start simulation\n",
      " \n",
      "-----------------------------------------------------------------------------------\n",
      "Right Left\n",
      "Current state in their representation =  002312\n",
      "Does the action make sense? Press Enter to execute\n",
      "REWARD =  [-0.5  -0.01 -0.01 -0.01]\n",
      "Is agent done?:  [False, False]\n",
      " \n",
      "-----------------------------------------------------------------------------------\n",
      "Up Up\n",
      "Current state in their representation =  012212\n",
      "Does the action make sense? Press Enter to execute\n",
      "REWARD =  [-0.01 -0.01 -0.01 -0.01]\n",
      "Is agent done?:  [False, False]\n",
      " \n",
      "-----------------------------------------------------------------------------------\n",
      "Up Up\n",
      "Current state in their representation =  012212\n",
      "Does the action make sense? Press Enter to execute\n",
      "REWARD =  [-0.01 -0.01 -0.01 -0.01]\n",
      "Is agent done?:  [False, False]\n",
      " \n",
      "-----------------------------------------------------------------------------------\n",
      "Right Right\n",
      "Current state in their representation =  012213\n",
      "Does the action make sense? Press Enter to execute\n",
      "REWARD =  [-0.5  -0.01 -0.01 -0.01]\n",
      "Is agent done?:  [False, False]\n",
      " \n",
      "-----------------------------------------------------------------------------------\n",
      "Right Right\n",
      "Current state in their representation =  022314\n",
      "Does the action make sense? Press Enter to execute\n",
      "REWARD =  [-0.5  -0.01 -0.01 -0.01]\n",
      "Is agent done?:  [False, False]\n",
      " \n",
      "-----------------------------------------------------------------------------------\n",
      "Down Right\n",
      "Current state in their representation =  032414\n",
      "Does the action make sense? Press Enter to execute\n",
      "REWARD =  [-0.5  -0.5  -0.01 -0.01]\n",
      "Is agent done?:  [True, True]\n"
     ]
    }
   ],
   "source": [
    "agent.analyse_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "772b444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_heatmap(values, half_state = \"2233\"):\n",
    "    \n",
    "    heatmap = np.zeros((5,5))\n",
    "    \n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "\n",
    "            state_encoding = state_to_encoding(str(i) + str(j) + half_state)\n",
    "            heatmap[i, j] = values[state_encoding]\n",
    "            \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7cf81af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11044947 0.86519987 0.69089671 0.2092728  0.43139052]\n",
      " [0.85289381 0.9769313  0.54766113 0.34265245 0.09834505]\n",
      " [0.63653723 0.09979531 0.42688338 0.22038347 0.95113388]\n",
      " [0.48131596 0.42086434 0.59614397 0.73815497 0.71963159]\n",
      " [0.9100647  0.61134894 0.90825994 0.98258555 0.79674944]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc35933bb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = value_heatmap(agent.values, half_state = \"2233\")\n",
    "print(heatmap)\n",
    "plt.imshow(heatmap, cmap=\"hot\", interpolation=\"nearest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement_Learning",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
